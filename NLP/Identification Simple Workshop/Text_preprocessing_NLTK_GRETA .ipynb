{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eccf7b97",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Dans toute tâche d'apprentissage automatique, le nettoyage ou le prétraitement des données est aussi important que la construction du modèle. Les données textuelles sont l'une des formes les moins structurées de données disponibles et lorsqu'il s'agit de traiter le langage humain, c'est trop complexe. \n",
    "Dans ce Brief nous allons travailler sur le prétraitement des données textuelles en utilisant [NLTK](http://www.nltk.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20d29f9",
   "metadata": {},
   "source": [
    "## Veille technologique: Natural Language processing (NLP)\n",
    "1- Les cas d'utlisation de NLP dans notre vie  \n",
    "2- Comment Fecebook, Google et Amazon utilisent NLP  \n",
    "3- Préparation des données textuelles  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df871de1",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041109d-7819-43b0-9018-139851f5b15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14aecb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importer les bibliothèques nécessaires\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f49b7e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dhimb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dhimb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Télécharger les données NLTK \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79a5cf",
   "metadata": {},
   "source": [
    "## Netoyage des données\n",
    "\n",
    "Dans cette partie nous allons utiliser [NLTK](http://www.nltk.org) pour nétoyer un texte de [wikipidéa](https://en.wikipedia.org/wiki/Natural_language_processing) sur la définition du NLP  \n",
    "\"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a6bf760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural language processing ( nlp ) is a subfield of linguistics , computer science , and artificial intelligence concerned with the interactions between computers and human language , in particular how to program computers to process and analyze large amounts of natural language data . the goal is a computer capable of `` understanding '' the contents of documents , including the contextual nuances of the language within them . the technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves .\n"
     ]
    }
   ],
   "source": [
    "#Lowercase: Mettre tout le texte en minuscule\n",
    "text= 'Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.'\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Convert each token into lowercase\n",
    "lowercase_tokens = [token.lower() for token in tokens]\n",
    "\n",
    "# Join the lowercase tokens back into a single string\n",
    "lowercase_text = ' '.join(lowercase_tokens)\n",
    "\n",
    "print(lowercase_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "463e7d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing NLP is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data The goal is a computer capable of `` understanding '' the contents of documents including the contextual nuances of the language within them The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves\n"
     ]
    }
   ],
   "source": [
    "#Supprimer les punctuation\n",
    "import string\n",
    "# Remove punctuation\n",
    "no_punctuation_tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "# Join the tokens back into a single string\n",
    "text_without_punctuation = ' '.join(no_punctuation_tokens)\n",
    "\n",
    "print(text_without_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e3cdd6",
   "metadata": {},
   "source": [
    "### Word Tokenization\n",
    "La tokénisation([Tokenize](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.casual)) consiste à diviser les chaînes de caractères en mots individuels sans blancs ni tabulations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4cf7be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.', 'The', 'goal', 'is', 'a', 'computer', 'capable', 'of', '``', 'understanding', \"''\", 'the', 'contents', 'of', 'documents', ',', 'including', 'the', 'contextual', 'nuances', 'of', 'the', 'language', 'within', 'them', '.', 'The', 'technology', 'can', 'then', 'accurately', 'extract', 'information', 'and', 'insights', 'contained', 'in', 'the', 'documents', 'as', 'well', 'as', 'categorize', 'and', 'organize', 'the', 'documents', 'themselves', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec18ba3",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "Les mots d'arrêt sont des mots qui n'ajoutent pas de sens significatif au texte. Utiliser NLTK pour lister les stop words et les supprimer du textes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18fcb732-0217-4102-ac5c-f76e96ce08e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'down', 'where', 'do', 'very', 'we', 'wouldn', 'too', 'into', 'shouldn', \"doesn't\", 'were', 'o', 're', 'such', 'don', 'below', 'only', 'myself', 'isn', 'own', 'or', 'our', 'doing', 'hers', 'd', 'now', 'are', 'did', 'up', 'until', 'y', 'should', 'not', 'am', \"hasn't\", 'they', 'a', 'few', 'each', 'her', 'at', 'him', 'ours', \"it's\", 'just', \"hadn't\", 'didn', 'nor', 'doesn', 'on', \"you'll\", 'he', 'other', 'ourselves', 's', 'having', 'i', 'here', 'you', 'this', 'can', 'about', \"that'll\", 'she', 'being', 'from', 'then', 'theirs', 'these', 'over', 'be', 'most', \"needn't\", \"weren't\", 'was', 'so', 'my', 'does', 'those', 'any', \"don't\", \"couldn't\", 'll', \"won't\", 'as', 'won', 'that', 'off', 'couldn', 'their', 'mightn', 'yours', 'above', 'its', 'before', 'been', \"wasn't\", 'why', 'in', \"should've\", 'mustn', 'further', 'have', 'wasn', 'whom', 'against', \"isn't\", \"mustn't\", 'both', 'yourself', 'himself', 'an', 'under', 'while', 'how', 'm', 'out', 'all', 'no', 'will', 'yourselves', \"you'd\", 'after', 've', 'what', 'has', \"shan't\", 'hadn', \"wouldn't\", 'itself', 'aren', 'if', 'hasn', 'shan', 'during', 'herself', 't', 'because', 'with', 'between', 'your', 'ma', \"aren't\", 'the', 'themselves', \"shouldn't\", 'again', 'by', 'when', 'is', 'had', 'of', \"you're\", 'needn', 'more', 'ain', 'same', \"haven't\", 'who', \"didn't\", 'haven', 'but', 'for', \"you've\", 'through', \"mightn't\", 'than', \"she's\", 'and', 'once', 'it', 'his', 'them', 'which', 'to', 'there', 'weren', 'some', 'me'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords # module for stop words that come with NLTK\n",
    "\n",
    "#récupérer les stopwords\n",
    "\n",
    "# Get English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad98be4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing ( NLP ) subfield linguistics , computer science , artificial intelligence concerned interactions computers human language , particular program computers process analyze large amounts natural language data . goal computer capable `` understanding '' contents documents , including contextual nuances language within . technology accurately extract information insights contained documents well categorize organize documents .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Supprimer les stopwords\n",
    "\n",
    "# Filter out the stop words\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Join the tokens back into a string\n",
    "filtered_text = ' '.join(filtered_tokens)\n",
    "\n",
    "print(filtered_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25588009",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "L'étymologie est le processus qui consiste à réduire les mots à leur racine, leur base ou leur forme ([Stemming](https://en.wikipedia.org/wiki/Stemming) )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ce47387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural --> natur\n",
      "language --> languag\n",
      "processing --> process\n",
      "( --> (\n",
      "NLP --> nlp\n",
      ") --> )\n",
      "is --> is\n",
      "a --> a\n",
      "subfield --> subfield\n",
      "of --> of\n",
      "linguistics --> linguist\n",
      ", --> ,\n",
      "computer --> comput\n",
      "science --> scienc\n",
      ", --> ,\n",
      "and --> and\n",
      "artificial --> artifici\n",
      "intelligence --> intellig\n",
      "concerned --> concern\n",
      "with --> with\n",
      "the --> the\n",
      "interactions --> interact\n",
      "between --> between\n",
      "computers --> comput\n",
      "and --> and\n",
      "human --> human\n",
      "language --> languag\n",
      ", --> ,\n",
      "in --> in\n",
      "particular --> particular\n",
      "how --> how\n",
      "to --> to\n",
      "program --> program\n",
      "computers --> comput\n",
      "to --> to\n",
      "process --> process\n",
      "and --> and\n",
      "analyze --> analyz\n",
      "large --> larg\n",
      "amounts --> amount\n",
      "of --> of\n",
      "natural --> natur\n",
      "language --> languag\n",
      "data --> data\n",
      ". --> .\n",
      "The --> the\n",
      "goal --> goal\n",
      "is --> is\n",
      "a --> a\n",
      "computer --> comput\n",
      "capable --> capabl\n",
      "of --> of\n",
      "`` --> ``\n",
      "understanding --> understand\n",
      "'' --> ''\n",
      "the --> the\n",
      "contents --> content\n",
      "of --> of\n",
      "documents --> document\n",
      ", --> ,\n",
      "including --> includ\n",
      "the --> the\n",
      "contextual --> contextu\n",
      "nuances --> nuanc\n",
      "of --> of\n",
      "the --> the\n",
      "language --> languag\n",
      "within --> within\n",
      "them --> them\n",
      ". --> .\n",
      "The --> the\n",
      "technology --> technolog\n",
      "can --> can\n",
      "then --> then\n",
      "accurately --> accur\n",
      "extract --> extract\n",
      "information --> inform\n",
      "and --> and\n",
      "insights --> insight\n",
      "contained --> contain\n",
      "in --> in\n",
      "the --> the\n",
      "documents --> document\n",
      "as --> as\n",
      "well --> well\n",
      "as --> as\n",
      "categorize --> categor\n",
      "and --> and\n",
      "organize --> organ\n",
      "the --> the\n",
      "documents --> document\n",
      "themselves --> themselv\n",
      ". --> .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "for token in tokens:\n",
    "    print(token + ' --> ' + stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e22285e2-ec8b-4b6f-b278-c46d09fc53fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural --> natur\n",
      "language --> languag\n",
      "processing --> process\n",
      "( --> (\n",
      "NLP --> nlp\n",
      ") --> )\n",
      "is --> is\n",
      "a --> a\n",
      "subfield --> subfield\n",
      "of --> of\n",
      "linguistics --> linguist\n",
      ", --> ,\n",
      "computer --> comput\n",
      "science --> scienc\n",
      ", --> ,\n",
      "and --> and\n",
      "artificial --> artifici\n",
      "intelligence --> intellig\n",
      "concerned --> concern\n",
      "with --> with\n",
      "the --> the\n",
      "interactions --> interact\n",
      "between --> between\n",
      "computers --> comput\n",
      "and --> and\n",
      "human --> human\n",
      "language --> languag\n",
      ", --> ,\n",
      "in --> in\n",
      "particular --> particular\n",
      "how --> how\n",
      "to --> to\n",
      "program --> program\n",
      "computers --> comput\n",
      "to --> to\n",
      "process --> process\n",
      "and --> and\n",
      "analyze --> analyz\n",
      "large --> larg\n",
      "amounts --> amount\n",
      "of --> of\n",
      "natural --> natur\n",
      "language --> languag\n",
      "data --> data\n",
      ". --> .\n",
      "The --> the\n",
      "goal --> goal\n",
      "is --> is\n",
      "a --> a\n",
      "computer --> comput\n",
      "capable --> capabl\n",
      "of --> of\n",
      "`` --> ``\n",
      "understanding --> understand\n",
      "'' --> ''\n",
      "the --> the\n",
      "contents --> content\n",
      "of --> of\n",
      "documents --> document\n",
      ", --> ,\n",
      "including --> includ\n",
      "the --> the\n",
      "contextual --> contextu\n",
      "nuances --> nuanc\n",
      "of --> of\n",
      "the --> the\n",
      "language --> languag\n",
      "within --> within\n",
      "them --> them\n",
      ". --> .\n",
      "The --> the\n",
      "technology --> technolog\n",
      "can --> can\n",
      "then --> then\n",
      "accurately --> accur\n",
      "extract --> extract\n",
      "information --> inform\n",
      "and --> and\n",
      "insights --> insight\n",
      "contained --> contain\n",
      "in --> in\n",
      "the --> the\n",
      "documents --> document\n",
      "as --> as\n",
      "well --> well\n",
      "as --> as\n",
      "categorize --> categor\n",
      "and --> and\n",
      "organize --> organ\n",
      "the --> the\n",
      "documents --> document\n",
      "themselves --> themselv\n",
      ". --> .\n"
     ]
    }
   ],
   "source": [
    "# Snowball Stemmer \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "\n",
    "for token in tokens:\n",
    "    print(token + ' --> ' + stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5418434",
   "metadata": {},
   "source": [
    "## Développement des fonctions\n",
    "\n",
    "Développer chaque étape du prétraitement du text dans une fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca52186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase: Mettre tout le texte en minuscule\n",
    "\n",
    "# Supprimer les punctuation\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "# Stopwords\n",
    "\n",
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3610110",
   "metadata": {},
   "source": [
    "# What about Twitter messages !! :)\n",
    "\n",
    "Dans cette partie nous allons appliquer les étapes de prétraitement de texte sur une base de données des messages Twitters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c886a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import matplotlib.pyplot as plt            # library for visualization\n",
    "import random                              # pseudo-random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d270188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\dhimb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "979b2607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "061ed30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m@Irenegilmour  Thanks for the flowers :) \n",
      "#flowers http://t.co/fh9a7oArCT\n",
      "\u001b[91mso many nasty, narrow minded people :(\n"
     ]
    }
   ],
   "source": [
    "#print positive in greeen\n",
    "print('\\033[92m' + all_positive_tweets[random.randint(0,5000)])\n",
    "\n",
    "# print negative in red\n",
    "print('\\033[91m' + all_negative_tweets[random.randint(0,5000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d66e283",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
